Fri Apr 19 18:14:21 MDT 2024
To request GPUs, add --gpus-per-node X or --gpus X, where X is the desired number of GPUs.
Job 2458008 running on hopper[002-005]
--------------------------------------------------------------------------
It appears as if there is not enough space for /tmp/ompi.hopper005.3792/jf.33176/0/shared_mem_cuda_pool.hopper005 (the shared-memory backing
file). It is likely that your MPI job will now either abort or experience
performance degradation.

  Local host:  hopper005
  Space Requested: 134217736 B
  Space Available: 7802880 B
--------------------------------------------------------------------------
[hopper005:3029774] create_and_attach: unable to create shared memory BTL coordinating strucure :: size 134217728 
================================================================================
HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018
Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK
Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK
Modified by Julien Langou, University of Colorado Denver
================================================================================

An explanation of the input/output parameters follows:
T/V    : Wall time / encoded variant.
N      : The order of the coefficient matrix A.
NB     : The partitioning blocking factor.
P      : The number of process rows.
Q      : The number of process columns.
Time   : Time in seconds to solve the linear system.
Gflops : Rate of execution for solving the linear system.

The following parameter values will be used:

N      :  190000 
NB     :     336 
PMAP   : Row-major process mapping
P      :      16 
Q      :       8 
PFACT  :   Right 
NBMIN  :       4 
NDIV   :       2 
RFACT  :   Right 
BCAST  :   2ring 
DEPTH  :       1 
SWAP   : Spread-roll (long)
L1     : transposed form
U      : transposed form
EQUIL  : yes
ALIGN  : 8 double precision words

--------------------------------------------------------------------------------

- The matrix A is randomly generated for each test.
- The following scaled residual check will be computed:
      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )
- The relative machine precision (eps) is taken to be               1.110223e-16
- Computational tests pass if scaled residuals are less than                16.0

slurmstepd: error: Detected 1 oom_kill event in StepId=2458008.0. Some of the step tasks have been OOM Killed.
srun: error: hopper005: task 97: Out Of Memory
[hopper005:3029774] *** An error occurred in MPI_Send
[hopper005:3029774] *** reported by process [2174222336,96]
[hopper005:3029774] *** on communicator MPI COMMUNICATOR 5 SPLIT FROM 3
[hopper005:3029774] *** MPI_ERR_OTHER: known error not in list
[hopper005:3029774] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper005:3029774] ***    and potentially your MPI job)
[hopper005:3029798] *** An error occurred in MPI_Send
[hopper005:3029798] *** reported by process [2174222336,120]
[hopper005:3029798] *** on communicator MPI COMMUNICATOR 5 SPLIT FROM 3
[hopper005:3029798] *** MPI_ERR_OTHER: known error not in list
[hopper005:3029798] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper005:3029798] ***    and potentially your MPI job)
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 2458008.0 ON hopper002 CANCELLED AT 2024-04-19T18:14:34 ***
srun: error: hopper004: tasks 64-66,68-73,77,79-80,82,87,89,92: Killed
srun: error: hopper003: tasks 33-39,45-46,48-49,52,55,57,60,62: Killed
srun: error: hopper002: tasks 0,10,12,14-16,18-19,21,24-25,27-31: Killed
srun: error: hopper003: tasks 32,40-44,47,50-51,53-54,56,58-59,61,63: Killed
srun: error: hopper004: tasks 67,74-76,78,81,83-86,88,90-91,93-95: Killed
srun: error: hopper002: tasks 1-9,11,13,17,20,22-23,26: Killed
Fri Apr 19 18:14:35 MDT 2024
